{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸŽ¯ Threshold Calibration for Prompt Output Evaluation\n",
        "\n",
        "This notebook implements a threshold calibration framework to determine if model outputs are semantically equivalent to expected outputs.\n",
        "\n",
        "**Models Used:**\n",
        "- **MiniLM** (`all-MiniLM-L6-v2`) - 384 dimensions, local\n",
        "- **MPNet** (`all-mpnet-base-v2`) - 768 dimensions, local\n",
        "- **OpenAI** (`text-embedding-3-large`) - 3072 dimensions, cloud API\n",
        "\n",
        "**Approach:**\n",
        "1. Create labeled calibration dataset (good/bad pairs)\n",
        "2. Compute optimal thresholds using ROC analysis\n",
        "3. Evaluate outputs using 3-model consensus voting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
        "from openai import OpenAI\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Initialize OpenAI client\n",
        "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Initialize local models\n",
        "print(\"Loading MiniLM model...\")\n",
        "model_minilm = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "print(\"Loading MPNet model...\")\n",
        "model_mpnet = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "print(\"âœ… All models loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“Š Calibration Dataset\n",
        "\n",
        "Define labeled pairs of outputs for calibration:\n",
        "- **Good pairs**: Semantically equivalent outputs (should PASS)\n",
        "- **Bad pairs**: Semantically different outputs (should FAIL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Good pairs: Semantically equivalent outputs that SHOULD match\n",
        "good_pairs = [\n",
        "    # Paraphrased sentences\n",
        "    (\"The capital of France is Paris.\", \"Paris is the capital city of France.\"),\n",
        "    (\"Python is a programming language.\", \"Python is a coding language used by developers.\"),\n",
        "    (\"Machine learning requires training data.\", \"ML models need data for training.\"),\n",
        "    (\"The weather is sunny today.\", \"Today is a beautiful sunny day.\"),\n",
        "    (\"I love drinking coffee in the morning.\", \"Coffee is my favorite morning drink.\"),\n",
        "    (\"The cat is sleeping on the couch.\", \"A feline is resting on the sofa.\"),\n",
        "    (\"Neural networks are used in AI.\", \"Artificial intelligence uses neural networks.\"),\n",
        "    (\"The meeting starts at 3 PM.\", \"The meeting is scheduled to begin at 3 PM.\"),\n",
        "    (\"She enjoys reading books.\", \"Reading books is something she loves.\"),\n",
        "    (\"The restaurant serves Italian food.\", \"Italian cuisine is served at the restaurant.\"),\n",
        "    (\"He drives a red car.\", \"His car is red in color.\"),\n",
        "    (\"The project deadline is Friday.\", \"Friday is when the project is due.\"),\n",
        "]\n",
        "\n",
        "# Bad pairs: Semantically different outputs that should NOT match\n",
        "bad_pairs = [\n",
        "    # Completely different topics\n",
        "    (\"The capital of France is Paris.\", \"Machine learning requires large datasets.\"),\n",
        "    (\"Python is a programming language.\", \"The weather is sunny today.\"),\n",
        "    (\"The cat is sleeping on the couch.\", \"Neural networks use backpropagation.\"),\n",
        "    (\"I love drinking coffee.\", \"The stock market closed higher today.\"),\n",
        "    (\"The meeting starts at 3 PM.\", \"Elephants are the largest land animals.\"),\n",
        "    (\"She enjoys reading books.\", \"The car needs an oil change.\"),\n",
        "    # Contradictory statements\n",
        "    (\"The weather is sunny today.\", \"It's raining heavily outside.\"),\n",
        "    (\"The project is complete.\", \"The project hasn't started yet.\"),\n",
        "    (\"He loves spicy food.\", \"He can't tolerate any spice.\"),\n",
        "    (\"The store is open.\", \"The store is closed for renovation.\"),\n",
        "    # Same topic but different meaning\n",
        "    (\"Paris is the capital of France.\", \"Berlin is the capital of Germany.\"),\n",
        "    (\"The cat is black.\", \"The dog is white.\"),\n",
        "]\n",
        "\n",
        "print(f\"   Calibration Dataset:\")\n",
        "print(f\"   Good pairs (should match): {len(good_pairs)}\")\n",
        "print(f\"   Bad pairs (should NOT match): {len(bad_pairs)}\")\n",
        "print(f\"   Total pairs: {len(good_pairs) + len(bad_pairs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”§ Similarity Computation Helper\n",
        "\n",
        "Create a unified function to compute similarity across all 3 models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_similarity(text1: str, text2: str, model_name: str = \"minilm\") -> float:\n",
        "    \"\"\"\n",
        "    Compute cosine similarity between two texts using specified model.\n",
        "    \n",
        "    Args:\n",
        "        text1: First text\n",
        "        text2: Second text\n",
        "        model_name: \"minilm\", \"mpnet\", or \"openai\"\n",
        "    \n",
        "    Returns:\n",
        "        Cosine similarity score between -1 and 1\n",
        "    \"\"\"\n",
        "    if model_name == \"openai\":\n",
        "        response = openai_client.embeddings.create(\n",
        "            model=\"text-embedding-3-large\",\n",
        "            input=[text1, text2]\n",
        "        )\n",
        "        embeddings = np.array([item.embedding for item in response.data])\n",
        "    else:\n",
        "        model = model_minilm if model_name == \"minilm\" else model_mpnet\n",
        "        embeddings = model.encode([text1, text2])\n",
        "    \n",
        "    similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
        "    return float(similarity)\n",
        "\n",
        "\n",
        "def get_batch_similarities(pairs: list[tuple[str, str]], model_name: str = \"minilm\") -> list[float]:\n",
        "    \"\"\"\n",
        "    Compute similarities for a batch of pairs efficiently.\n",
        "    \n",
        "    Args:\n",
        "        pairs: List of (text1, text2) tuples\n",
        "        model_name: \"minilm\", \"mpnet\", or \"openai\"\n",
        "    \n",
        "    Returns:\n",
        "        List of similarity scores\n",
        "    \"\"\"\n",
        "    if model_name == \"openai\":\n",
        "        # Batch all texts for efficiency\n",
        "        all_texts = []\n",
        "        for t1, t2 in pairs:\n",
        "            all_texts.extend([t1, t2])\n",
        "        \n",
        "        response = openai_client.embeddings.create(\n",
        "            model=\"text-embedding-3-large\",\n",
        "            input=all_texts\n",
        "        )\n",
        "        all_embeddings = np.array([item.embedding for item in response.data])\n",
        "        \n",
        "        similarities = []\n",
        "        for i in range(0, len(all_embeddings), 2):\n",
        "            sim = cosine_similarity([all_embeddings[i]], [all_embeddings[i+1]])[0][0]\n",
        "            similarities.append(float(sim))\n",
        "        return similarities\n",
        "    else:\n",
        "        model = model_minilm if model_name == \"minilm\" else model_mpnet\n",
        "        \n",
        "        # Encode all texts at once\n",
        "        all_texts = []\n",
        "        for t1, t2 in pairs:\n",
        "            all_texts.extend([t1, t2])\n",
        "        \n",
        "        all_embeddings = model.encode(all_texts)\n",
        "        \n",
        "        similarities = []\n",
        "        for i in range(0, len(all_embeddings), 2):\n",
        "            sim = cosine_similarity([all_embeddings[i]], [all_embeddings[i+1]])[0][0]\n",
        "            similarities.append(float(sim))\n",
        "        return similarities\n",
        "\n",
        "\n",
        "# Test the helper\n",
        "test_sim = get_similarity(\n",
        "    \"Python is a programming language.\",\n",
        "    \"Python is a coding language.\",\n",
        "    model_name=\"minilm\"\n",
        ")\n",
        "print(f\"Helper function working! Test similarity: {test_sim:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“ˆ Compute Similarities for All Pairs\n",
        "\n",
        "Batch compute similarities for the entire calibration dataset across all 3 models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute similarities for all models\n",
        "print(\"Computing similarities for calibration dataset...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "model_names = [\"minilm\", \"mpnet\", \"openai\"]\n",
        "all_similarities = {}\n",
        "\n",
        "for model_name in model_names:\n",
        "    print(f\"  Processing {model_name.upper()}...\")\n",
        "    good_sims = get_batch_similarities(good_pairs, model_name)\n",
        "    bad_sims = get_batch_similarities(bad_pairs, model_name)\n",
        "    \n",
        "    all_similarities[model_name] = {\n",
        "        \"good\": good_sims,\n",
        "        \"bad\": bad_sims\n",
        "    }\n",
        "    \n",
        "    print(f\"    Good pairs: mean={np.mean(good_sims):.3f}, std={np.std(good_sims):.3f}\")\n",
        "    print(f\"    Bad pairs:  mean={np.mean(bad_sims):.3f}, std={np.std(bad_sims):.3f}\")\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(\"All similarities computed!\")\n",
        "\n",
        "# Create summary DataFrame\n",
        "summary_data = []\n",
        "for model_name in model_names:\n",
        "    good_sims = all_similarities[model_name][\"good\"]\n",
        "    bad_sims = all_similarities[model_name][\"bad\"]\n",
        "    summary_data.append({\n",
        "        \"Model\": model_name.upper(),\n",
        "        \"Good Mean\": f\"{np.mean(good_sims):.3f}\",\n",
        "        \"Good Std\": f\"{np.std(good_sims):.3f}\",\n",
        "        \"Bad Mean\": f\"{np.mean(bad_sims):.3f}\",\n",
        "        \"Bad Std\": f\"{np.std(bad_sims):.3f}\",\n",
        "        \"Separation\": f\"{np.mean(good_sims) - np.mean(bad_sims):.3f}\"\n",
        "    })\n",
        "\n",
        "df_summary = pd.DataFrame(summary_data)\n",
        "print(\"\\nSimilarity Summary:\")\n",
        "print(df_summary.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ¯ Threshold Calibration\n",
        "\n",
        "Find optimal thresholds using ROC analysis and F1 optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calibrate_threshold(good_scores: list[float], bad_scores: list[float]) -> dict:\n",
        "    \"\"\"\n",
        "    Find optimal threshold using ROC and F1 analysis.\n",
        "    \n",
        "    Args:\n",
        "        good_scores: Similarity scores for pairs that should match (label=1)\n",
        "        bad_scores: Similarity scores for pairs that should NOT match (label=0)\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with optimal thresholds and statistics\n",
        "    \"\"\"\n",
        "    # Combine scores and create labels\n",
        "    all_scores = np.array(good_scores + bad_scores)\n",
        "    all_labels = np.array([1] * len(good_scores) + [0] * len(bad_scores))\n",
        "    \n",
        "    # ROC curve analysis\n",
        "    fpr, tpr, roc_thresholds = roc_curve(all_labels, all_scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    \n",
        "    # Find ROC-optimal threshold (maximizes Youden's J = TPR - FPR)\n",
        "    youden_j = tpr - fpr\n",
        "    roc_optimal_idx = np.argmax(youden_j)\n",
        "    roc_threshold = roc_thresholds[roc_optimal_idx]\n",
        "    \n",
        "    # Precision-Recall curve for F1 optimization\n",
        "    precision, recall, pr_thresholds = precision_recall_curve(all_labels, all_scores)\n",
        "    \n",
        "    # Calculate F1 scores (avoid division by zero)\n",
        "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
        "    f1_optimal_idx = np.argmax(f1_scores[:-1])  # Last element is sentinel\n",
        "    f1_threshold = pr_thresholds[f1_optimal_idx]\n",
        "    \n",
        "    return {\n",
        "        \"roc_threshold\": float(roc_threshold),\n",
        "        \"f1_threshold\": float(f1_threshold),\n",
        "        \"roc_auc\": float(roc_auc),\n",
        "        \"best_f1\": float(f1_scores[f1_optimal_idx]),\n",
        "        \"good_mean\": float(np.mean(good_scores)),\n",
        "        \"good_std\": float(np.std(good_scores)),\n",
        "        \"bad_mean\": float(np.mean(bad_scores)),\n",
        "        \"bad_std\": float(np.std(bad_scores)),\n",
        "        # Store curves for visualization\n",
        "        \"roc_curve\": {\"fpr\": fpr, \"tpr\": tpr, \"thresholds\": roc_thresholds},\n",
        "        \"pr_curve\": {\"precision\": precision, \"recall\": recall, \"thresholds\": pr_thresholds}\n",
        "    }\n",
        "\n",
        "\n",
        "# Calibrate thresholds for each model\n",
        "print(\"Calibrating thresholds for each model...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "calibration_results = {}\n",
        "for model_name in model_names:\n",
        "    good_sims = all_similarities[model_name][\"good\"]\n",
        "    bad_sims = all_similarities[model_name][\"bad\"]\n",
        "    \n",
        "    calibration_results[model_name] = calibrate_threshold(good_sims, bad_sims)\n",
        "    \n",
        "    result = calibration_results[model_name]\n",
        "    print(f\"\\n{model_name.upper()}:\")\n",
        "    print(f\"   ROC-Optimal Threshold: {result['roc_threshold']:.4f}\")\n",
        "    print(f\"   F1-Optimal Threshold:  {result['f1_threshold']:.4f}\")\n",
        "    print(f\"   ROC AUC Score:         {result['roc_auc']:.4f}\")\n",
        "    print(f\"   Best F1 Score:         {result['best_f1']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "\n",
        "# Create thresholds summary table\n",
        "thresholds_data = []\n",
        "for model_name in model_names:\n",
        "    r = calibration_results[model_name]\n",
        "    thresholds_data.append({\n",
        "        \"Model\": model_name.upper(),\n",
        "        \"ROC Threshold\": f\"{r['roc_threshold']:.4f}\",\n",
        "        \"F1 Threshold\": f\"{r['f1_threshold']:.4f}\",\n",
        "        \"ROC AUC\": f\"{r['roc_auc']:.4f}\",\n",
        "        \"Best F1\": f\"{r['best_f1']:.4f}\"\n",
        "    })\n",
        "\n",
        "df_thresholds = pd.DataFrame(thresholds_data)\n",
        "print(\"\\nCalibrated Thresholds Summary:\")\n",
        "print(df_thresholds.to_string(index=False))\n",
        "\n",
        "# Store thresholds in a simple dict for evaluation functions\n",
        "thresholds = {\n",
        "    model: calibration_results[model][\"roc_threshold\"]\n",
        "    for model in model_names\n",
        "}\n",
        "print(f\"\\nUsing ROC-optimal thresholds: {thresholds}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âœ… Evaluation Functions\n",
        "\n",
        "Functions to evaluate if model outputs match expected outputs using calibrated thresholds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_output(expected: str, actual: str, thresholds: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Evaluate if actual output matches expected using all 3 models.\n",
        "    \n",
        "    Args:\n",
        "        expected: The expected/reference output\n",
        "        actual: The actual model output to evaluate\n",
        "        thresholds: Dict of {model_name: threshold} for each model\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with results for each model and consensus\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    \n",
        "    for model_name in [\"minilm\", \"mpnet\", \"openai\"]:\n",
        "        similarity = get_similarity(expected, actual, model_name)\n",
        "        threshold = thresholds[model_name]\n",
        "        passed = similarity >= threshold\n",
        "        \n",
        "        results[model_name] = {\n",
        "            \"similarity\": similarity,\n",
        "            \"threshold\": threshold,\n",
        "            \"passed\": passed\n",
        "        }\n",
        "    \n",
        "    # Consensus: majority vote (2 out of 3)\n",
        "    passed_count = sum(1 for r in results.values() if r[\"passed\"])\n",
        "    results[\"consensus\"] = {\n",
        "        \"passed\": passed_count >= 2,\n",
        "        \"votes\": f\"{passed_count}/3\",\n",
        "        \"verdict\": \"PASS\" if passed_count >= 2 else \"FAIL\"\n",
        "    }\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "def evaluate_with_confidence(expected: str, actual: str, thresholds: dict, \n",
        "                             margin: float = 0.1) -> dict:\n",
        "    \"\"\"\n",
        "    Evaluate with confidence bands to flag uncertain cases.\n",
        "    \n",
        "    Args:\n",
        "        expected: The expected/reference output\n",
        "        actual: The actual model output to evaluate\n",
        "        thresholds: Dict of {model_name: threshold} for each model\n",
        "        margin: Uncertainty margin around threshold (default 0.1)\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with results including confidence levels\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    \n",
        "    for model_name in [\"minilm\", \"mpnet\", \"openai\"]:\n",
        "        similarity = get_similarity(expected, actual, model_name)\n",
        "        threshold = thresholds[model_name]\n",
        "        \n",
        "        # Determine confidence level\n",
        "        if similarity >= threshold + margin:\n",
        "            verdict = \"HIGH_PASS\"\n",
        "            confidence = \"high\"\n",
        "        elif similarity >= threshold:\n",
        "            verdict = \"PASS\"\n",
        "            confidence = \"medium\"\n",
        "        elif similarity >= threshold - margin:\n",
        "            verdict = \"UNCERTAIN\"\n",
        "            confidence = \"low\"\n",
        "        else:\n",
        "            verdict = \"FAIL\"\n",
        "            confidence = \"high\"\n",
        "        \n",
        "        results[model_name] = {\n",
        "            \"similarity\": similarity,\n",
        "            \"threshold\": threshold,\n",
        "            \"verdict\": verdict,\n",
        "            \"confidence\": confidence\n",
        "        }\n",
        "    \n",
        "    # Determine overall verdict\n",
        "    verdicts = [r[\"verdict\"] for r in results.values() if r[\"verdict\"] != \"consensus\"]\n",
        "    \n",
        "    high_pass_count = sum(1 for v in verdicts if v == \"HIGH_PASS\")\n",
        "    pass_count = sum(1 for v in verdicts if v in [\"HIGH_PASS\", \"PASS\"])\n",
        "    uncertain_count = sum(1 for v in verdicts if v == \"UNCERTAIN\")\n",
        "    fail_count = sum(1 for v in verdicts if v == \"FAIL\")\n",
        "    \n",
        "    if high_pass_count >= 2 or pass_count == 3:\n",
        "        overall_verdict = \"HIGH CONFIDENCE PASS\"\n",
        "    elif pass_count >= 2:\n",
        "        overall_verdict = \"PASS\"\n",
        "    elif fail_count >= 2:\n",
        "        overall_verdict = \"FAIL\"\n",
        "    elif uncertain_count > 0:\n",
        "        overall_verdict = \"UNCERTAIN - Manual review recommended\"\n",
        "    else:\n",
        "        overall_verdict = \"FAIL\"\n",
        "    \n",
        "    results[\"overall\"] = {\n",
        "        \"verdict\": overall_verdict,\n",
        "        \"pass_votes\": f\"{pass_count}/3\",\n",
        "        \"uncertain_count\": uncertain_count\n",
        "    }\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "def print_evaluation_result(expected: str, actual: str, result: dict, with_confidence: bool = False):\n",
        "    \"\"\"Pretty print evaluation results.\"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Expected: {expected[:60]}{'...' if len(expected) > 60 else ''}\")\n",
        "    print(f\"Actual:   {actual[:60]}{'...' if len(actual) > 60 else ''}\")\n",
        "    print(f\"{'â”€'*70}\")\n",
        "    \n",
        "    for model_name in [\"minilm\", \"mpnet\", \"openai\"]:\n",
        "        r = result[model_name]\n",
        "        sim = r[\"similarity\"]\n",
        "        thresh = r[\"threshold\"]\n",
        "        \n",
        "        if with_confidence:\n",
        "            verdict = r[\"verdict\"]\n",
        "            if verdict == \"HIGH_PASS\":\n",
        "                status = \"HIGH PASS\"\n",
        "            elif verdict == \"PASS\":\n",
        "                status = \"PASS\"\n",
        "            elif verdict == \"UNCERTAIN\":\n",
        "                status = \"UNCERTAIN\"\n",
        "            else:\n",
        "                status = \"FAIL\"\n",
        "        else:\n",
        "            status = \"PASS\" if r[\"passed\"] else \"FAIL\"\n",
        "        \n",
        "        print(f\"   {model_name.upper():8} | Sim: {sim:.4f} | Thresh: {thresh:.4f} | {status}\")\n",
        "    \n",
        "    print(f\"{'â”€'*70}\")\n",
        "    if with_confidence:\n",
        "        print(f\"   OVERALL: {result['overall']['verdict']}\")\n",
        "    else:\n",
        "        print(f\"   CONSENSUS: {result['consensus']['verdict']} ({result['consensus']['votes']} models passed)\")\n",
        "    print(f\"{'='*70}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“Š Visualization\n",
        "\n",
        "Visualize the similarity distributions and calibrated thresholds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create visualization of similarity distributions\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "colors = {\"minilm\": \"#2ecc71\", \"mpnet\": \"#3498db\", \"openai\": \"#9b59b6\"}\n",
        "\n",
        "for idx, model_name in enumerate(model_names):\n",
        "    ax = axes[idx]\n",
        "    \n",
        "    good_sims = all_similarities[model_name][\"good\"]\n",
        "    bad_sims = all_similarities[model_name][\"bad\"]\n",
        "    threshold = thresholds[model_name]\n",
        "    \n",
        "    # Plot histograms\n",
        "    ax.hist(good_sims, bins=10, alpha=0.7, label=\"Good pairs (should match)\", \n",
        "            color=\"#2ecc71\", edgecolor=\"white\")\n",
        "    ax.hist(bad_sims, bins=10, alpha=0.7, label=\"Bad pairs (should NOT match)\", \n",
        "            color=\"#e74c3c\", edgecolor=\"white\")\n",
        "    \n",
        "    # Plot threshold line\n",
        "    ax.axvline(x=threshold, color=\"#2c3e50\", linestyle=\"--\", linewidth=2, \n",
        "               label=f\"Threshold: {threshold:.3f}\")\n",
        "    \n",
        "    ax.set_xlabel(\"Cosine Similarity\", fontsize=11)\n",
        "    ax.set_ylabel(\"Frequency\", fontsize=11)\n",
        "    ax.set_title(f\"{model_name.upper()}\", fontsize=13, fontweight=\"bold\")\n",
        "    ax.legend(loc=\"upper left\", fontsize=9)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle(\"Similarity Distributions with Calibrated Thresholds\", fontsize=14, fontweight=\"bold\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ§ª Interactive Testing\n",
        "\n",
        "Test the evaluation framework with example cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Case 1: Clear PASS - Semantically equivalent\n",
        "print(\"TEST CASE 1: Clear PASS (semantically equivalent)\")\n",
        "expected1 = \"Machine learning models require training data to learn patterns.\"\n",
        "actual1 = \"ML algorithms need datasets for training to identify patterns.\"\n",
        "\n",
        "result1 = evaluate_output(expected1, actual1, thresholds)\n",
        "print_evaluation_result(expected1, actual1, result1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Case 2: Clear FAIL - Completely different topics\n",
        "print(\"TEST CASE 2: Clear FAIL (completely different)\")\n",
        "expected2 = \"The capital of France is Paris.\"\n",
        "actual2 = \"Neural networks use backpropagation for training.\"\n",
        "\n",
        "result2 = evaluate_output(expected2, actual2, thresholds)\n",
        "print_evaluation_result(expected2, actual2, result2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Case 3: Borderline case - Use confidence evaluation\n",
        "print(\"TEST CASE 3: Borderline case (with confidence)\")\n",
        "expected3 = \"Python is widely used in data science.\"\n",
        "actual3 = \"Data analysis often uses programming languages.\"\n",
        "\n",
        "result3 = evaluate_with_confidence(expected3, actual3, thresholds)\n",
        "print_evaluation_result(expected3, actual3, result3, with_confidence=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Case 4: Same topic, contradictory - Should FAIL\n",
        "print(\"TEST CASE 4: Same topic but contradictory\")\n",
        "expected4 = \"The weather is sunny and warm today.\"\n",
        "actual4 = \"It's cold and raining outside.\"\n",
        "\n",
        "result4 = evaluate_with_confidence(expected4, actual4, thresholds)\n",
        "print_evaluation_result(expected4, actual4, result4, with_confidence=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive: Test your own inputs!\n",
        "print(\"TRY YOUR OWN INPUTS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Modify these to test your own cases\n",
        "your_expected = \"The restaurant serves delicious Italian pasta.\"\n",
        "your_actual = \"They make great Italian food at this place.\"\n",
        "\n",
        "print(f\"\\nYour expected: {your_expected}\")\n",
        "print(f\"Your actual:   {your_actual}\")\n",
        "\n",
        "your_result = evaluate_with_confidence(your_expected, your_actual, thresholds)\n",
        "print_evaluation_result(your_expected, your_actual, your_result, with_confidence=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“‹ Summary\n",
        "\n",
        "This notebook provides:\n",
        "\n",
        "1. **Calibration Dataset**: Labeled good/bad pairs for threshold training\n",
        "2. **Optimal Thresholds**: Computed using ROC analysis for each model\n",
        "3. **Evaluation Functions**:\n",
        "   - `evaluate_output()` - Basic pass/fail with 3-model consensus\n",
        "   - `evaluate_with_confidence()` - Adds uncertainty zones for borderline cases\n",
        "4. **Visualizations**: Distribution histograms and ROC curves\n",
        "\n",
        "**Usage:**\n",
        "```python\n",
        "# Basic evaluation\n",
        "result = evaluate_output(expected, actual, thresholds)\n",
        "\n",
        "# With confidence bands\n",
        "result = evaluate_with_confidence(expected, actual, thresholds)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot ROC curves for all models\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "for idx, model_name in enumerate(model_names):\n",
        "    ax = axes[idx]\n",
        "    \n",
        "    roc_data = calibration_results[model_name][\"roc_curve\"]\n",
        "    fpr = roc_data[\"fpr\"]\n",
        "    tpr = roc_data[\"tpr\"]\n",
        "    roc_auc = calibration_results[model_name][\"roc_auc\"]\n",
        "    \n",
        "    ax.plot(fpr, tpr, linewidth=2, label=f\"ROC (AUC = {roc_auc:.3f})\")\n",
        "    ax.plot([0, 1], [0, 1], \"k--\", linewidth=1, label=\"Random\")\n",
        "    ax.fill_between(fpr, tpr, alpha=0.3)\n",
        "    \n",
        "    ax.set_xlabel(\"False Positive Rate\", fontsize=11)\n",
        "    ax.set_ylabel(\"True Positive Rate\", fontsize=11)\n",
        "    ax.set_title(f\"{model_name.upper()} - ROC Curve\", fontsize=13, fontweight=\"bold\")\n",
        "    ax.legend(loc=\"lower right\", fontsize=10)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_xlim([0, 1])\n",
        "    ax.set_ylim([0, 1.05])\n",
        "\n",
        "plt.suptitle(\"ROC Curves for Threshold Calibration\", fontsize=14, fontweight=\"bold\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
