{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Statistical Threshold Calibration for AI Assistant Outputs\n",
        "\n",
        "This notebook implements a **statistical quality gate** for LLM outputs by:\n",
        "\n",
        "1. **Generating** 100 outputs from the same prompt using GPT-4o-mini\n",
        "2. **Embedding** all outputs using text-embedding-3-small\n",
        "3. **Calibrating** a threshold using KDE and bootstrap confidence intervals\n",
        "4. **Runtime evaluation** with accept/retry/escalate logic\n",
        "5. **Drift detection** for ongoing monitoring\n",
        "\n",
        "**Models Used:**\n",
        "- `gpt-4o-mini` - Cost-effective generation with good quality\n",
        "- `text-embedding-3-small` - Efficient embeddings for similarity computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI client initialized successfully!\n",
            "Using models: gpt-4o-mini (generation), text-embedding-3-small (embeddings)\n"
          ]
        }
      ],
      "source": [
        "# Core imports\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.stats import gaussian_kde, ks_2samp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "print(\"OpenAI client initialized successfully!\")\n",
        "print(f\"Using models: gpt-4o-mini (generation), text-embedding-3-small (embeddings)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration Constants\n",
        "\n",
        "Define the key parameters for our calibration experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration loaded:\n",
            "  - Outputs to generate: 100\n",
            "  - Temperature: 0.2\n",
            "  - Threshold percentile: 5th\n",
            "  - Bootstrap iterations: 1000\n",
            "  - Drift window size: 50\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "# Number of outputs to generate for calibration\n",
        "N_OUTPUTS = 100\n",
        "\n",
        "# Temperature for sampling (higher = more diversity)\n",
        "TEMPERATURE = 0.2\n",
        "\n",
        "# Model configurations\n",
        "GENERATION_MODEL = \"gpt-4o-mini\"\n",
        "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
        "\n",
        "# Calibration parameters\n",
        "THRESHOLD_PERCENTILE = 5  # Use 5th percentile as threshold (95% of good outputs above)\n",
        "BOOTSTRAP_ITERATIONS = 1000  # Number of bootstrap resamples for CI\n",
        "CONFIDENCE_LEVEL = 0.95  # 95% confidence interval\n",
        "\n",
        "# Drift detection parameters\n",
        "DRIFT_WINDOW_SIZE = 50  # Rolling window for drift monitoring\n",
        "DRIFT_ALPHA = 0.05  # Significance level for KS test\n",
        "DRIFT_Z_THRESHOLD = 2  # Z-score threshold for mean shift detection\n",
        "\n",
        "print(f\"Configuration loaded:\")\n",
        "print(f\"  - Outputs to generate: {N_OUTPUTS}\")\n",
        "print(f\"  - Temperature: {TEMPERATURE}\")\n",
        "print(f\"  - Threshold percentile: {THRESHOLD_PERCENTILE}th\")\n",
        "print(f\"  - Bootstrap iterations: {BOOTSTRAP_ITERATIONS}\")\n",
        "print(f\"  - Drift window size: {DRIFT_WINDOW_SIZE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prompt Definitions\n",
        "\n",
        "Define the system prompt (simulating your professional tone) and user prompt for the calibration task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System Prompt:\n",
            "--------------------------------------------------\n",
            "You are an AI assistant that answers like a senior AI architect.\n",
            "\n",
            "Style guidelines:\n",
            "- Structured and organized responses\n",
            "- Use bullet points for clarity\n",
            "- Be precise and technical\n",
            "- Minimal fluff - get to the point\n",
            "- Actionable recommendations\n",
            "- Professional tone throughout\n",
            "\n",
            "User Prompt:\n",
            "--------------------------------------------------\n",
            "Explain how to implement a threshold-based evaluation system to validate\n",
            "LLM outputs using embeddings and cosine similarity.\n",
            "\n",
            "Provide:\n",
            "1. Key steps in the implementation\n",
            "2. Best practices to consider\n",
            "3. Potential pitfalls to avoid\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# PROMPT DEFINITIONS\n",
        "# =============================================================================\n",
        "\n",
        "# System prompt - Simulates professional AI architect tone\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are an AI assistant that answers like a senior AI architect.\n",
        "\n",
        "Style guidelines:\n",
        "- Structured and organized responses\n",
        "- Use bullet points for clarity\n",
        "- Be precise and technical\n",
        "- Minimal fluff - get to the point\n",
        "- Actionable recommendations\n",
        "- Professional tone throughout\n",
        "\"\"\"\n",
        "\n",
        "# User prompt - The question we'll generate multiple responses for\n",
        "USER_PROMPT = \"\"\"\n",
        "Explain how to implement a threshold-based evaluation system to validate\n",
        "LLM outputs using embeddings and cosine similarity.\n",
        "\n",
        "Provide:\n",
        "1. Key steps in the implementation\n",
        "2. Best practices to consider\n",
        "3. Potential pitfalls to avoid\n",
        "\"\"\"\n",
        "\n",
        "print(\"System Prompt:\")\n",
        "print(\"-\" * 50)\n",
        "print(SYSTEM_PROMPT.strip())\n",
        "print()\n",
        "print(\"User Prompt:\")\n",
        "print(\"-\" * 50)\n",
        "print(USER_PROMPT.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generation Functions\n",
        "\n",
        "Helper functions to generate outputs using GPT-4o-mini."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing generation function...\n",
            "Test output length: 3208 characters\n",
            "First 200 characters: ### Implementation of a Threshold-Based Evaluation System for LLM Outputs\n",
            "\n",
            "#### 1. Key Steps in the Implementation\n",
            "\n",
            "- **Data Preparation**\n",
            "  - Collect a dataset of reference outputs (ground truth) for...\n"
          ]
        }
      ],
      "source": [
        "def generate_output(system_prompt: str, user_prompt: str, temp: float = 0.9) -> str:\n",
        "    \"\"\"\n",
        "    Generate a single output using GPT-4o-mini.\n",
        "    \n",
        "    Args:\n",
        "        system_prompt: The system prompt defining assistant behavior\n",
        "        user_prompt: The user's question/request\n",
        "        temp: Temperature for sampling (higher = more diverse)\n",
        "    \n",
        "    Returns:\n",
        "        Generated text response\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=GENERATION_MODEL,\n",
        "        temperature=temp,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "def generate_batch_outputs(system_prompt: str, user_prompt: str, n: int, temp: float = 0.9) -> list[str]:\n",
        "    \"\"\"\n",
        "    Generate N outputs for the same prompt with progress tracking.\n",
        "    \n",
        "    Args:\n",
        "        system_prompt: The system prompt defining assistant behavior\n",
        "        user_prompt: The user's question/request\n",
        "        n: Number of outputs to generate\n",
        "        temp: Temperature for sampling\n",
        "    \n",
        "    Returns:\n",
        "        List of generated text responses\n",
        "    \"\"\"\n",
        "    outputs = []\n",
        "    for _ in tqdm(range(n), desc=\"Generating outputs\"):\n",
        "        output = generate_output(system_prompt, user_prompt, temp)\n",
        "        outputs.append(output)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "# Test the generation function\n",
        "print(\"Testing generation function...\")\n",
        "test_output = generate_output(SYSTEM_PROMPT, USER_PROMPT, TEMPERATURE)\n",
        "print(f\"Test output length: {len(test_output)} characters\")\n",
        "print(f\"First 200 characters: {test_output[:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Embedding Functions\n",
        "\n",
        "Helper functions to embed texts using text-embedding-3-small."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing embedding function...\n",
            "Embedding shape: (1, 1536)\n",
            "Embedding dimension: 1536\n"
          ]
        }
      ],
      "source": [
        "def embed_texts(texts: list[str]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Embed a list of texts using text-embedding-3-small.\n",
        "    \n",
        "    Args:\n",
        "        texts: List of text strings to embed\n",
        "    \n",
        "    Returns:\n",
        "        numpy array of shape (n_texts, embedding_dim)\n",
        "    \"\"\"\n",
        "    response = client.embeddings.create(\n",
        "        model=EMBEDDING_MODEL,\n",
        "        input=texts\n",
        "    )\n",
        "    return np.array([d.embedding for d in response.data])\n",
        "\n",
        "\n",
        "def compute_centroid(embeddings: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute the centroid (mean) of a set of embeddings.\n",
        "    \n",
        "    Args:\n",
        "        embeddings: numpy array of shape (n, dim)\n",
        "    \n",
        "    Returns:\n",
        "        Centroid vector of shape (dim,)\n",
        "    \"\"\"\n",
        "    return embeddings.mean(axis=0)\n",
        "\n",
        "\n",
        "# Test the embedding function\n",
        "print(\"Testing embedding function...\")\n",
        "test_embeddings = embed_texts([test_output])\n",
        "print(f\"Embedding shape: {test_embeddings.shape}\")\n",
        "print(f\"Embedding dimension: {test_embeddings.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Generate Calibration Outputs\n",
        "\n",
        "Generate N outputs from the same prompt to build our calibration distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating 100 outputs for calibration...\n",
            "This may take a few minutes...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating outputs:  18%|█▊        | 18/100 [04:20<20:01, 14.65s/it]"
          ]
        }
      ],
      "source": [
        "# Generate N outputs for calibration\n",
        "print(f\"Generating {N_OUTPUTS} outputs for calibration...\")\n",
        "print(f\"This may take a few minutes...\")\n",
        "\n",
        "outputs = generate_batch_outputs(SYSTEM_PROMPT, USER_PROMPT, N_OUTPUTS, TEMPERATURE)\n",
        "\n",
        "print(f\"\\nGeneration complete!\")\n",
        "print(f\"Total outputs: {len(outputs)}\")\n",
        "print(f\"Average output length: {np.mean([len(o) for o in outputs]):.0f} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Create Reference and Compute Embeddings\n",
        "\n",
        "Select a reference output and embed all outputs for similarity computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select reference output (first output as baseline)\n",
        "# In production, you might use a human-curated \"gold\" reference\n",
        "reference_output = outputs[0]\n",
        "\n",
        "print(\"Reference output (first 300 chars):\")\n",
        "print(\"-\" * 50)\n",
        "print(reference_output[:300] + \"...\")\n",
        "print()\n",
        "\n",
        "# Embed all outputs\n",
        "print(\"Embedding all outputs...\")\n",
        "embeddings = embed_texts(outputs)\n",
        "print(f\"Embeddings shape: {embeddings.shape}\")\n",
        "\n",
        "# Get reference embedding\n",
        "ref_embedding = embeddings[0]\n",
        "\n",
        "# Compute centroid embedding (for self-consistency metric)\n",
        "centroid = compute_centroid(embeddings)\n",
        "\n",
        "print(f\"Reference embedding shape: {ref_embedding.shape}\")\n",
        "print(f\"Centroid embedding shape: {centroid.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Compute Similarity Scores\n",
        "\n",
        "Calculate cosine similarities to reference and centroid for all outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute similarity to reference\n",
        "sims_to_ref = cosine_similarity(embeddings, ref_embedding.reshape(1, -1)).flatten()\n",
        "\n",
        "# Compute similarity to centroid (self-consistency metric)\n",
        "sims_to_centroid = cosine_similarity(embeddings, centroid.reshape(1, -1)).flatten()\n",
        "\n",
        "print(\"Similarity to Reference:\")\n",
        "print(f\"  Mean: {np.mean(sims_to_ref):.4f}\")\n",
        "print(f\"  Std:  {np.std(sims_to_ref):.4f}\")\n",
        "print(f\"  Min:  {np.min(sims_to_ref):.4f}\")\n",
        "print(f\"  Max:  {np.max(sims_to_ref):.4f}\")\n",
        "print()\n",
        "print(\"Similarity to Centroid:\")\n",
        "print(f\"  Mean: {np.mean(sims_to_centroid):.4f}\")\n",
        "print(f\"  Std:  {np.std(sims_to_centroid):.4f}\")\n",
        "print(f\"  Min:  {np.min(sims_to_centroid):.4f}\")\n",
        "print(f\"  Max:  {np.max(sims_to_centroid):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Visualize Similarity Distributions\n",
        "\n",
        "Examine the distribution of similarities to understand the output consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create side-by-side histograms\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Histogram: Similarity to Reference\n",
        "ax1 = axes[0]\n",
        "ax1.hist(sims_to_ref, bins=20, color='#3498db', edgecolor='white', alpha=0.8)\n",
        "ax1.axvline(np.mean(sims_to_ref), color='#e74c3c', linestyle='--', linewidth=2, label=f'Mean: {np.mean(sims_to_ref):.4f}')\n",
        "ax1.set_xlabel('Cosine Similarity', fontsize=12)\n",
        "ax1.set_ylabel('Frequency', fontsize=12)\n",
        "ax1.set_title('Similarity to Reference Output', fontsize=14, fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Histogram: Similarity to Centroid\n",
        "ax2 = axes[1]\n",
        "ax2.hist(sims_to_centroid, bins=20, color='#2ecc71', edgecolor='white', alpha=0.8)\n",
        "ax2.axvline(np.mean(sims_to_centroid), color='#e74c3c', linestyle='--', linewidth=2, label=f'Mean: {np.mean(sims_to_centroid):.4f}')\n",
        "ax2.set_xlabel('Cosine Similarity', fontsize=12)\n",
        "ax2.set_ylabel('Frequency', fontsize=12)\n",
        "ax2.set_title('Similarity to Centroid (Self-Consistency)', fontsize=14, fontweight='bold')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: KDE Modeling\n",
        "\n",
        "Fit a Kernel Density Estimate to model the smooth distribution of similarity scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit KDE to similarity-to-reference distribution\n",
        "kde = gaussian_kde(sims_to_ref)\n",
        "\n",
        "# Create smooth x-axis for plotting\n",
        "x_range = np.linspace(min(sims_to_ref) - 0.05, max(sims_to_ref) + 0.05, 200)\n",
        "kde_values = kde(x_range)\n",
        "\n",
        "# Plot KDE\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Histogram with density\n",
        "ax.hist(sims_to_ref, bins=20, density=True, color='#3498db', edgecolor='white', alpha=0.6, label='Histogram')\n",
        "\n",
        "# KDE curve\n",
        "ax.plot(x_range, kde_values, color='#e74c3c', linewidth=2.5, label='KDE (Gaussian)')\n",
        "\n",
        "# Mark mean\n",
        "ax.axvline(np.mean(sims_to_ref), color='#2c3e50', linestyle='--', linewidth=2, label=f'Mean: {np.mean(sims_to_ref):.4f}')\n",
        "\n",
        "ax.set_xlabel('Cosine Similarity', fontsize=12)\n",
        "ax.set_ylabel('Density', fontsize=12)\n",
        "ax.set_title('KDE of Similarity to Reference', fontsize=14, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"KDE fitted successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Threshold Calibration\n",
        "\n",
        "Calculate the threshold at the configured percentile (default: 5th percentile).\n",
        "\n",
        "This means 95% of acceptable outputs will be **above** this threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate threshold at different percentiles\n",
        "threshold_1pct = np.percentile(sims_to_ref, 1)\n",
        "threshold_5pct = np.percentile(sims_to_ref, 5)\n",
        "threshold_10pct = np.percentile(sims_to_ref, 10)\n",
        "\n",
        "# Use configured percentile as our main threshold\n",
        "threshold = np.percentile(sims_to_ref, THRESHOLD_PERCENTILE)\n",
        "\n",
        "print(\"Threshold Calibration Results:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"  1st percentile threshold:  {threshold_1pct:.4f}\")\n",
        "print(f\"  5th percentile threshold:  {threshold_5pct:.4f} {'<-- SELECTED' if THRESHOLD_PERCENTILE == 5 else ''}\")\n",
        "print(f\"  10th percentile threshold: {threshold_10pct:.4f}\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"\\nCalibrated Threshold: {threshold:.4f}\")\n",
        "print(f\"Interpretation: {100 - THRESHOLD_PERCENTILE}% of outputs are above this threshold\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Bootstrap Confidence Interval\n",
        "\n",
        "Compute a confidence interval for our threshold using bootstrap resampling.\n",
        "\n",
        "This gives us statistical rigor and uncertainty bounds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bootstrap resampling for confidence interval\n",
        "np.random.seed(42)  # For reproducibility\n",
        "\n",
        "bootstrap_thresholds = []\n",
        "\n",
        "print(f\"Running {BOOTSTRAP_ITERATIONS} bootstrap iterations...\")\n",
        "for _ in tqdm(range(BOOTSTRAP_ITERATIONS)):\n",
        "    # Resample with replacement\n",
        "    sample = np.random.choice(sims_to_ref, size=len(sims_to_ref), replace=True)\n",
        "    # Calculate threshold for this sample\n",
        "    sample_threshold = np.percentile(sample, THRESHOLD_PERCENTILE)\n",
        "    bootstrap_thresholds.append(sample_threshold)\n",
        "\n",
        "bootstrap_thresholds = np.array(bootstrap_thresholds)\n",
        "\n",
        "# Calculate confidence interval\n",
        "alpha = 1 - CONFIDENCE_LEVEL\n",
        "ci_low = np.percentile(bootstrap_thresholds, (alpha / 2) * 100)\n",
        "ci_high = np.percentile(bootstrap_thresholds, (1 - alpha / 2) * 100)\n",
        "\n",
        "print(f\"\\nBootstrap Results:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"  Point Estimate (Threshold): {threshold:.4f}\")\n",
        "print(f\"  {CONFIDENCE_LEVEL * 100:.0f}% Confidence Interval: [{ci_low:.4f}, {ci_high:.4f}]\")\n",
        "print(f\"  CI Width: {ci_high - ci_low:.4f}\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize bootstrap distribution\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "ax.hist(bootstrap_thresholds, bins=30, color='#9b59b6', edgecolor='white', alpha=0.7)\n",
        "ax.axvline(threshold, color='#e74c3c', linestyle='-', linewidth=2.5, label=f'Point Estimate: {threshold:.4f}')\n",
        "ax.axvline(ci_low, color='#2c3e50', linestyle='--', linewidth=2, label=f'CI Lower: {ci_low:.4f}')\n",
        "ax.axvline(ci_high, color='#2c3e50', linestyle='--', linewidth=2, label=f'CI Upper: {ci_high:.4f}')\n",
        "\n",
        "# Shade CI region\n",
        "ax.axvspan(ci_low, ci_high, alpha=0.2, color='#3498db', label=f'{CONFIDENCE_LEVEL * 100:.0f}% CI')\n",
        "\n",
        "ax.set_xlabel('Threshold Value', fontsize=12)\n",
        "ax.set_ylabel('Frequency', fontsize=12)\n",
        "ax.set_title('Bootstrap Distribution of Threshold Estimate', fontsize=14, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Runtime Evaluation Functions\n",
        "\n",
        "Create functions to evaluate new outputs against the calibrated threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_output_score(text: str, ref_emb: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Compute similarity score for a single output.\n",
        "    \n",
        "    Args:\n",
        "        text: The output text to evaluate\n",
        "        ref_emb: Reference embedding to compare against\n",
        "    \n",
        "    Returns:\n",
        "        Cosine similarity score\n",
        "    \"\"\"\n",
        "    emb = embed_texts([text])[0]\n",
        "    score = cosine_similarity(emb.reshape(1, -1), ref_emb.reshape(1, -1))[0][0]\n",
        "    return score\n",
        "\n",
        "\n",
        "def evaluate_output(text: str, ref_emb: np.ndarray, threshold: float) -> tuple[bool, float]:\n",
        "    \"\"\"\n",
        "    Evaluate if an output passes the threshold.\n",
        "    \n",
        "    Args:\n",
        "        text: The output text to evaluate\n",
        "        ref_emb: Reference embedding to compare against\n",
        "        threshold: Minimum similarity threshold\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (passed: bool, score: float)\n",
        "    \"\"\"\n",
        "    score = evaluate_output_score(text, ref_emb)\n",
        "    passed = score >= threshold\n",
        "    return passed, score\n",
        "\n",
        "\n",
        "def evaluate_with_retry(\n",
        "    system_prompt: str,\n",
        "    user_prompt: str,\n",
        "    ref_emb: np.ndarray,\n",
        "    threshold: float,\n",
        "    max_retries: int = 3,\n",
        "    temp: float = 0.9\n",
        ") -> tuple[str, float, str, int]:\n",
        "    \"\"\"\n",
        "    Generate output with retry logic if below threshold.\n",
        "    \n",
        "    Args:\n",
        "        system_prompt: System prompt for generation\n",
        "        user_prompt: User prompt for generation\n",
        "        ref_emb: Reference embedding\n",
        "        threshold: Minimum similarity threshold\n",
        "        max_retries: Maximum number of attempts\n",
        "        temp: Temperature for generation\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (output, score, decision, attempts)\n",
        "        Decision is one of: \"ACCEPT\", \"ESCALATE\"\n",
        "    \"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        output = generate_output(system_prompt, user_prompt, temp)\n",
        "        passed, score = evaluate_output(output, ref_emb, threshold)\n",
        "        \n",
        "        if passed:\n",
        "            return output, score, \"ACCEPT\", attempt + 1\n",
        "    \n",
        "    # All retries exhausted\n",
        "    return output, score, \"ESCALATE\", max_retries\n",
        "\n",
        "\n",
        "print(\"Runtime evaluation functions defined:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Test Runtime Evaluation\n",
        "\n",
        "Test the evaluation function with a new generated output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate a new test output\n",
        "print(\"Generating new test output...\")\n",
        "new_output = generate_output(SYSTEM_PROMPT, USER_PROMPT, TEMPERATURE)\n",
        "\n",
        "# Evaluate against threshold\n",
        "passed, score = evaluate_output(new_output, ref_embedding, threshold)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"RUNTIME EVALUATION RESULT\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Score:     {score:.4f}\")\n",
        "print(f\"Threshold: {threshold:.4f}\")\n",
        "print(f\"CI:        [{ci_low:.4f}, {ci_high:.4f}]\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "if passed:\n",
        "    print(f\"Decision:  ACCEPT\")\n",
        "else:\n",
        "    print(f\"Decision:  RETRY (score below threshold)\")\n",
        "    \n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the retry logic (with higher threshold to force retries)\n",
        "print(\"Testing accept/retry/escalate logic...\")\n",
        "print(\"(Using slightly higher threshold to demonstrate retry behavior)\")\n",
        "\n",
        "# Use a threshold that will likely trigger at least one retry\n",
        "test_threshold = np.percentile(sims_to_ref, 50)  # 50th percentile = median\n",
        "\n",
        "output, score, decision, attempts = evaluate_with_retry(\n",
        "    SYSTEM_PROMPT, USER_PROMPT, ref_embedding, \n",
        "    test_threshold, max_retries=3\n",
        ")\n",
        "\n",
        "print(f\"\\nResult:\")\n",
        "print(f\"  Decision: {decision}\")\n",
        "print(f\"  Attempts: {attempts}\")\n",
        "print(f\"  Final Score: {score:.4f}\")\n",
        "print(f\"  Test Threshold: {test_threshold:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9b: Hypothesis Testing for Output Evaluation\n",
        "\n",
        "Add formal statistical hypothesis testing to make accept/reject decisions more rigorous.\n",
        "\n",
        "### Hypothesis Framework\n",
        "\n",
        "```\n",
        "H0 (Null Hypothesis):     The output similarity score is at or below the threshold (score ≤ T)\n",
        "H1 (Alternative):         The output similarity score is above the threshold (score > T)\n",
        "\n",
        "Decision Rule:\n",
        "- p-value < α  →  Reject H0  →  ACCEPT output (statistically confident it exceeds threshold)\n",
        "- p-value ≥ α  →  Fail to reject H0  →  RETRY/ESCALATE (insufficient evidence)\n",
        "```\n",
        "\n",
        "### Why This Approach?\n",
        "\n",
        "- **Conservative by default**: Assumes output is NOT acceptable until proven otherwise\n",
        "- **Controls false acceptance rate**: Setting α=0.05 means at most 5% chance of accepting bad output\n",
        "- **Two evaluation modes**:\n",
        "  - **Single output**: Z-test using calibration distribution parameters\n",
        "  - **Batch output**: T-test for multiple outputs (more robust for small samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import norm, ttest_1samp\n",
        "\n",
        "def evaluate_output_hypothesis(\n",
        "    text: str, \n",
        "    ref_emb: np.ndarray, \n",
        "    threshold: float,\n",
        "    baseline_mean: float,\n",
        "    baseline_std: float,\n",
        "    alpha: float = 0.05\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Evaluate a single output using hypothesis testing.\n",
        "    \n",
        "    Hypothesis:\n",
        "        H0: score <= threshold (output is NOT acceptable)\n",
        "        H1: score > threshold (output IS acceptable)\n",
        "    \n",
        "    Uses z-test with calibration distribution parameters.\n",
        "    \n",
        "    Args:\n",
        "        text: Output text to evaluate\n",
        "        ref_emb: Reference embedding\n",
        "        threshold: Calibrated threshold value\n",
        "        baseline_mean: Mean from calibration distribution\n",
        "        baseline_std: Std from calibration distribution\n",
        "        alpha: Significance level (default 0.05)\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with score, z_score, p_value, and decision\n",
        "    \"\"\"\n",
        "    # Compute similarity score\n",
        "    score = evaluate_output_score(text, ref_emb)\n",
        "    \n",
        "    # Z-score: how many standard deviations above the threshold\n",
        "    z_score = (score - threshold) / baseline_std\n",
        "    \n",
        "    # One-tailed p-value (upper tail)\n",
        "    # P(Z > z_score) under H0\n",
        "    p_value = 1 - norm.cdf(z_score)\n",
        "    \n",
        "    # Decision based on significance level\n",
        "    reject_h0 = p_value < alpha\n",
        "    decision = \"ACCEPT\" if reject_h0 else \"RETRY\"\n",
        "    \n",
        "    return {\n",
        "        \"score\": float(score),\n",
        "        \"threshold\": float(threshold),\n",
        "        \"z_score\": float(z_score),\n",
        "        \"p_value\": float(p_value),\n",
        "        \"alpha\": alpha,\n",
        "        \"reject_h0\": reject_h0,\n",
        "        \"decision\": decision,\n",
        "        \"interpretation\": f\"{'Reject' if reject_h0 else 'Fail to reject'} H0 at α={alpha}\"\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"Single output hypothesis test function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_batch_hypothesis(\n",
        "    scores: np.ndarray,\n",
        "    threshold: float,\n",
        "    alpha: float = 0.05\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Evaluate a batch of outputs using one-sample t-test.\n",
        "    \n",
        "    Hypothesis:\n",
        "        H0: mean(scores) <= threshold (batch is NOT acceptable on average)\n",
        "        H1: mean(scores) > threshold (batch IS acceptable on average)\n",
        "    \n",
        "    Uses t-test which is more robust for small samples.\n",
        "    \n",
        "    Args:\n",
        "        scores: Array of similarity scores\n",
        "        threshold: Calibrated threshold value\n",
        "        alpha: Significance level (default 0.05)\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with statistics, p_value, and decision\n",
        "    \"\"\"\n",
        "    # One-sample t-test\n",
        "    t_stat, p_value_two_tailed = ttest_1samp(scores, threshold)\n",
        "    \n",
        "    # Convert to one-tailed (we only care if mean > threshold)\n",
        "    # If t_stat > 0, mean is above threshold, use half the two-tailed p-value\n",
        "    # If t_stat < 0, mean is below threshold, p-value for upper tail is 1 - half\n",
        "    if t_stat > 0:\n",
        "        p_value = p_value_two_tailed / 2\n",
        "    else:\n",
        "        p_value = 1 - p_value_two_tailed / 2\n",
        "    \n",
        "    # Decision\n",
        "    reject_h0 = p_value < alpha\n",
        "    decision = \"ACCEPT\" if reject_h0 else \"ESCALATE\"\n",
        "    \n",
        "    return {\n",
        "        \"n_samples\": len(scores),\n",
        "        \"mean_score\": float(np.mean(scores)),\n",
        "        \"std_score\": float(np.std(scores, ddof=1)),  # Sample std\n",
        "        \"threshold\": float(threshold),\n",
        "        \"t_statistic\": float(t_stat),\n",
        "        \"p_value\": float(p_value),\n",
        "        \"alpha\": alpha,\n",
        "        \"reject_h0\": reject_h0,\n",
        "        \"decision\": decision,\n",
        "        \"interpretation\": f\"{'Reject' if reject_h0 else 'Fail to reject'} H0 at α={alpha}\"\n",
        "    }\n",
        "\n",
        "\n",
        "def generate_and_test_batch(\n",
        "    system_prompt: str,\n",
        "    user_prompt: str,\n",
        "    ref_emb: np.ndarray,\n",
        "    threshold: float,\n",
        "    n_samples: int = 10,\n",
        "    temp: float = 0.9,\n",
        "    alpha: float = 0.05\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Generate a batch of outputs and run hypothesis test.\n",
        "    \n",
        "    Args:\n",
        "        system_prompt: System prompt for generation\n",
        "        user_prompt: User prompt for generation\n",
        "        ref_emb: Reference embedding\n",
        "        threshold: Calibrated threshold\n",
        "        n_samples: Number of outputs to generate\n",
        "        temp: Temperature for generation\n",
        "        alpha: Significance level\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with outputs, scores, and hypothesis test results\n",
        "    \"\"\"\n",
        "    # Generate outputs\n",
        "    outputs = generate_batch_outputs(system_prompt, user_prompt, n_samples, temp)\n",
        "    \n",
        "    # Embed and compute scores\n",
        "    embeddings = embed_texts(outputs)\n",
        "    scores = cosine_similarity(embeddings, ref_emb.reshape(1, -1)).flatten()\n",
        "    \n",
        "    # Run hypothesis test\n",
        "    test_result = evaluate_batch_hypothesis(scores, threshold, alpha)\n",
        "    \n",
        "    return {\n",
        "        \"outputs\": outputs,\n",
        "        \"scores\": scores,\n",
        "        \"test_result\": test_result\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"Batch hypothesis test functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Store baseline statistics for hypothesis testing\n",
        "# (These are also used later for drift detection)\n",
        "baseline_stats = {\n",
        "    \"scores\": sims_to_ref.copy(),\n",
        "    \"mean\": np.mean(sims_to_ref),\n",
        "    \"std\": np.std(sims_to_ref),\n",
        "    \"kde\": kde,\n",
        "    \"threshold\": threshold,\n",
        "    \"ci_low\": ci_low,\n",
        "    \"ci_high\": ci_high\n",
        "}\n",
        "\n",
        "print(\"Baseline Statistics for Hypothesis Testing:\")\n",
        "print(f\"  Mean: {baseline_stats['mean']:.4f}\")\n",
        "print(f\"  Std:  {baseline_stats['std']:.4f}\")\n",
        "print(f\"  N samples: {len(baseline_stats['scores'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test: Single Output Hypothesis Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test single output hypothesis evaluation\n",
        "print(\"Generating new output for hypothesis test...\")\n",
        "test_output_hyp = generate_output(SYSTEM_PROMPT, USER_PROMPT, TEMPERATURE)\n",
        "\n",
        "# Run hypothesis test\n",
        "hyp_result = evaluate_output_hypothesis(\n",
        "    text=test_output_hyp,\n",
        "    ref_emb=ref_embedding,\n",
        "    threshold=threshold,\n",
        "    baseline_mean=baseline_stats[\"mean\"],\n",
        "    baseline_std=baseline_stats[\"std\"],\n",
        "    alpha=0.05\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"SINGLE OUTPUT HYPOTHESIS TEST RESULT\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"H0: Score ≤ {hyp_result['threshold']:.4f} (output not acceptable)\")\n",
        "print(f\"H1: Score > {hyp_result['threshold']:.4f} (output acceptable)\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"Observed Score:  {hyp_result['score']:.4f}\")\n",
        "print(f\"Threshold:       {hyp_result['threshold']:.4f}\")\n",
        "print(f\"Z-Score:         {hyp_result['z_score']:.4f}\")\n",
        "print(f\"P-Value:         {hyp_result['p_value']:.6f}\")\n",
        "print(f\"Alpha (α):       {hyp_result['alpha']}\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"Decision:        {hyp_result['interpretation']}\")\n",
        "print(f\"Action:          {hyp_result['decision']}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if hyp_result['reject_h0']:\n",
        "    print(\"\\nInterpretation: There is statistically significant evidence\")\n",
        "    print(f\"that the output exceeds the threshold (p={hyp_result['p_value']:.4f} < α={hyp_result['alpha']})\")\n",
        "else:\n",
        "    print(\"\\nInterpretation: Insufficient evidence to conclude the output\")\n",
        "    print(f\"exceeds the threshold (p={hyp_result['p_value']:.4f} ≥ α={hyp_result['alpha']})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test: Batch Output Hypothesis Evaluation\n",
        "\n",
        "Generate multiple outputs and use t-test for more robust statistical inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test batch hypothesis evaluation\n",
        "N_BATCH_SAMPLES = 15  # Generate 15 outputs for the test\n",
        "\n",
        "print(f\"Generating {N_BATCH_SAMPLES} outputs for batch hypothesis test...\")\n",
        "batch_result = generate_and_test_batch(\n",
        "    system_prompt=SYSTEM_PROMPT,\n",
        "    user_prompt=USER_PROMPT,\n",
        "    ref_emb=ref_embedding,\n",
        "    threshold=threshold,\n",
        "    n_samples=N_BATCH_SAMPLES,\n",
        "    temp=TEMPERATURE,\n",
        "    alpha=0.05\n",
        ")\n",
        "\n",
        "test_result = batch_result[\"test_result\"]\n",
        "batch_scores = batch_result[\"scores\"]\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"BATCH OUTPUT HYPOTHESIS TEST RESULT (T-Test)\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"H0: Mean(scores) ≤ {test_result['threshold']:.4f} (batch not acceptable on average)\")\n",
        "print(f\"H1: Mean(scores) > {test_result['threshold']:.4f} (batch acceptable on average)\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"Sample Size (n):    {test_result['n_samples']}\")\n",
        "print(f\"Mean Score:         {test_result['mean_score']:.4f}\")\n",
        "print(f\"Std Score:          {test_result['std_score']:.4f}\")\n",
        "print(f\"Threshold:          {test_result['threshold']:.4f}\")\n",
        "print(f\"T-Statistic:        {test_result['t_statistic']:.4f}\")\n",
        "print(f\"P-Value:            {test_result['p_value']:.6f}\")\n",
        "print(f\"Alpha (α):          {test_result['alpha']}\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"Decision:           {test_result['interpretation']}\")\n",
        "print(f\"Action:             {test_result['decision']}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Show individual scores\n",
        "print(f\"\\nIndividual Scores (sorted):\")\n",
        "for i, score in enumerate(sorted(batch_scores, reverse=True)):\n",
        "    status = \"above\" if score >= threshold else \"BELOW\"\n",
        "    print(f\"  {i+1:2}. {score:.4f} ({status} threshold)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization: Hypothesis Test Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize hypothesis test results\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# === Left Plot: Single Output Z-Test Visualization ===\n",
        "ax1 = axes[0]\n",
        "\n",
        "# Plot the calibration distribution\n",
        "x_range = np.linspace(\n",
        "    min(baseline_stats[\"scores\"]) - 0.1, \n",
        "    max(baseline_stats[\"scores\"]) + 0.1, \n",
        "    200\n",
        ")\n",
        "kde_baseline = gaussian_kde(baseline_stats[\"scores\"])\n",
        "ax1.plot(x_range, kde_baseline(x_range), color='#3498db', linewidth=2, label='Calibration Distribution')\n",
        "ax1.fill_between(x_range, kde_baseline(x_range), alpha=0.3, color='#3498db')\n",
        "\n",
        "# Mark threshold\n",
        "ax1.axvline(threshold, color='#e74c3c', linestyle='--', linewidth=2.5, label=f'Threshold: {threshold:.4f}')\n",
        "\n",
        "# Shade rejection region (above threshold = ACCEPT zone)\n",
        "x_accept = x_range[x_range >= threshold]\n",
        "ax1.fill_between(x_accept, kde_baseline(x_accept), alpha=0.4, color='#2ecc71', label='Accept Region')\n",
        "\n",
        "# Mark the test output score\n",
        "ax1.axvline(hyp_result['score'], color='#9b59b6', linestyle='-', linewidth=3, \n",
        "            label=f'Test Score: {hyp_result[\"score\"]:.4f}')\n",
        "ax1.scatter([hyp_result['score']], [kde_baseline(hyp_result['score'])], \n",
        "            color='#9b59b6', s=150, zorder=5, marker='*')\n",
        "\n",
        "# Add p-value annotation\n",
        "ax1.annotate(f'p = {hyp_result[\"p_value\"]:.4f}\\n{hyp_result[\"decision\"]}', \n",
        "             xy=(hyp_result['score'], kde_baseline(hyp_result['score'])),\n",
        "             xytext=(hyp_result['score'] + 0.02, kde_baseline(hyp_result['score']) + 1),\n",
        "             fontsize=11, fontweight='bold',\n",
        "             arrowprops=dict(arrowstyle='->', color='#9b59b6'))\n",
        "\n",
        "ax1.set_xlabel('Cosine Similarity', fontsize=12)\n",
        "ax1.set_ylabel('Density', fontsize=12)\n",
        "ax1.set_title('Single Output Hypothesis Test (Z-Test)', fontsize=14, fontweight='bold')\n",
        "ax1.legend(loc='upper left', fontsize=9)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# === Right Plot: Batch T-Test Visualization ===\n",
        "ax2 = axes[1]\n",
        "\n",
        "# Histogram of batch scores\n",
        "ax2.hist(batch_scores, bins=10, color='#9b59b6', edgecolor='white', alpha=0.7, \n",
        "         label=f'Batch Scores (n={len(batch_scores)})', density=True)\n",
        "\n",
        "# Mark threshold\n",
        "ax2.axvline(threshold, color='#e74c3c', linestyle='--', linewidth=2.5, \n",
        "            label=f'Threshold: {threshold:.4f}')\n",
        "\n",
        "# Mark batch mean\n",
        "ax2.axvline(test_result['mean_score'], color='#2ecc71', linestyle='-', linewidth=2.5, \n",
        "            label=f'Batch Mean: {test_result[\"mean_score\"]:.4f}')\n",
        "\n",
        "# Shade based on decision\n",
        "if test_result['reject_h0']:\n",
        "    ax2.axvspan(threshold, max(batch_scores) + 0.05, alpha=0.2, color='#2ecc71')\n",
        "else:\n",
        "    ax2.axvspan(min(batch_scores) - 0.05, threshold, alpha=0.2, color='#e74c3c')\n",
        "\n",
        "# Add annotation\n",
        "result_text = f't = {test_result[\"t_statistic\"]:.3f}\\np = {test_result[\"p_value\"]:.4f}\\n{test_result[\"decision\"]}'\n",
        "ax2.annotate(result_text, xy=(0.95, 0.95), xycoords='axes fraction',\n",
        "             fontsize=11, fontweight='bold', ha='right', va='top',\n",
        "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "ax2.set_xlabel('Cosine Similarity', fontsize=12)\n",
        "ax2.set_ylabel('Density', fontsize=12)\n",
        "ax2.set_title('Batch Hypothesis Test (T-Test)', fontsize=14, fontweight='bold')\n",
        "ax2.legend(loc='upper left', fontsize=9)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"HYPOTHESIS TESTING SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Single Output Test:  {hyp_result['decision']} (p = {hyp_result['p_value']:.4f})\")\n",
        "print(f\"Batch Test (n={test_result['n_samples']}):     {test_result['decision']} (p = {test_result['p_value']:.4f})\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Drift Detection\n",
        "\n",
        "Implement drift detection to monitor when the model behavior changes and recalibration may be needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# baseline_stats was already computed in the hypothesis testing section\n",
        "# Verify it's available for drift detection\n",
        "print(\"Baseline Statistics (from earlier calibration):\")\n",
        "print(f\"  Mean: {baseline_stats['mean']:.4f}\")\n",
        "print(f\"  Std:  {baseline_stats['std']:.4f}\")\n",
        "print(f\"  N samples: {len(baseline_stats['scores'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_drift_ks(baseline_scores: np.ndarray, new_scores: np.ndarray, alpha: float = 0.05) -> dict:\n",
        "    \"\"\"\n",
        "    Detect drift using the Kolmogorov-Smirnov test.\n",
        "    \n",
        "    The KS test compares two distributions and detects if they're statistically different.\n",
        "    \n",
        "    Args:\n",
        "        baseline_scores: Array of baseline similarity scores\n",
        "        new_scores: Array of new similarity scores\n",
        "        alpha: Significance level (default 0.05)\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with drift_detected, ks_statistic, p_value\n",
        "    \"\"\"\n",
        "    statistic, p_value = ks_2samp(baseline_scores, new_scores)\n",
        "    drift_detected = p_value < alpha\n",
        "    \n",
        "    return {\n",
        "        \"drift_detected\": drift_detected,\n",
        "        \"ks_statistic\": statistic,\n",
        "        \"p_value\": p_value,\n",
        "        \"alpha\": alpha\n",
        "    }\n",
        "\n",
        "\n",
        "def detect_drift_mean(\n",
        "    baseline_scores: np.ndarray, \n",
        "    new_scores: np.ndarray, \n",
        "    z_threshold: float = 2.0\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Detect drift by checking if the mean has shifted significantly.\n",
        "    \n",
        "    Args:\n",
        "        baseline_scores: Array of baseline similarity scores\n",
        "        new_scores: Array of new similarity scores\n",
        "        z_threshold: Number of standard deviations for drift detection\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with drift_detected, z_score, baseline_mean, new_mean\n",
        "    \"\"\"\n",
        "    baseline_mean = np.mean(baseline_scores)\n",
        "    baseline_std = np.std(baseline_scores)\n",
        "    new_mean = np.mean(new_scores)\n",
        "    \n",
        "    # Calculate z-score of the difference\n",
        "    z_score = abs(new_mean - baseline_mean) / (baseline_std / np.sqrt(len(new_scores)))\n",
        "    drift_detected = z_score > z_threshold\n",
        "    \n",
        "    return {\n",
        "        \"drift_detected\": drift_detected,\n",
        "        \"z_score\": z_score,\n",
        "        \"baseline_mean\": baseline_mean,\n",
        "        \"new_mean\": new_mean,\n",
        "        \"z_threshold\": z_threshold\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"Drift detection functions defined:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DriftMonitor:\n",
        "    \"\"\"\n",
        "    Rolling window drift monitor for production use.\n",
        "    \n",
        "    Continuously monitors similarity scores and alerts when drift is detected.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self, \n",
        "        baseline_scores: np.ndarray, \n",
        "        window_size: int = 50,\n",
        "        ks_alpha: float = 0.05,\n",
        "        z_threshold: float = 2.0\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the drift monitor.\n",
        "        \n",
        "        Args:\n",
        "            baseline_scores: Calibration baseline scores\n",
        "            window_size: Size of rolling window for new scores\n",
        "            ks_alpha: Significance level for KS test\n",
        "            z_threshold: Z-score threshold for mean shift\n",
        "        \"\"\"\n",
        "        self.baseline = baseline_scores.copy()\n",
        "        self.window_size = window_size\n",
        "        self.ks_alpha = ks_alpha\n",
        "        self.z_threshold = z_threshold\n",
        "        self.recent_scores = []\n",
        "        self.drift_history = []\n",
        "        \n",
        "    def add_score(self, score: float):\n",
        "        \"\"\"Add a new score to the monitoring window.\"\"\"\n",
        "        self.recent_scores.append(score)\n",
        "        if len(self.recent_scores) > self.window_size:\n",
        "            self.recent_scores.pop(0)\n",
        "            \n",
        "    def check_drift(self) -> dict:\n",
        "        \"\"\"\n",
        "        Check for drift in the current window.\n",
        "        \n",
        "        Returns:\n",
        "            Dictionary with drift status and details\n",
        "        \"\"\"\n",
        "        if len(self.recent_scores) < self.window_size:\n",
        "            return {\n",
        "                \"status\": \"INSUFFICIENT_DATA\",\n",
        "                \"message\": f\"Need {self.window_size - len(self.recent_scores)} more samples\",\n",
        "                \"drift_detected\": None\n",
        "            }\n",
        "        \n",
        "        recent_array = np.array(self.recent_scores)\n",
        "        \n",
        "        # Run both drift tests\n",
        "        ks_result = detect_drift_ks(self.baseline, recent_array, self.ks_alpha)\n",
        "        mean_result = detect_drift_mean(self.baseline, recent_array, self.z_threshold)\n",
        "        \n",
        "        # Combined drift detection\n",
        "        drift_detected = ks_result[\"drift_detected\"] or mean_result[\"drift_detected\"]\n",
        "        \n",
        "        result = {\n",
        "            \"status\": \"DRIFT_DETECTED\" if drift_detected else \"STABLE\",\n",
        "            \"drift_detected\": drift_detected,\n",
        "            \"ks_test\": ks_result,\n",
        "            \"mean_shift\": mean_result,\n",
        "            \"window_size\": len(self.recent_scores),\n",
        "            \"recent_mean\": np.mean(recent_array),\n",
        "            \"recent_std\": np.std(recent_array)\n",
        "        }\n",
        "        \n",
        "        self.drift_history.append(result)\n",
        "        return result\n",
        "    \n",
        "    def get_summary(self) -> dict:\n",
        "        \"\"\"Get summary of drift monitoring.\"\"\"\n",
        "        return {\n",
        "            \"baseline_mean\": np.mean(self.baseline),\n",
        "            \"baseline_std\": np.std(self.baseline),\n",
        "            \"current_window_size\": len(self.recent_scores),\n",
        "            \"target_window_size\": self.window_size,\n",
        "            \"drift_checks_performed\": len(self.drift_history),\n",
        "            \"drifts_detected\": sum(1 for h in self.drift_history if h.get(\"drift_detected\", False))\n",
        "        }\n",
        "\n",
        "\n",
        "# Create drift monitor instance\n",
        "drift_monitor = DriftMonitor(\n",
        "    baseline_stats[\"scores\"],\n",
        "    window_size=DRIFT_WINDOW_SIZE,\n",
        "    ks_alpha=DRIFT_ALPHA,\n",
        "    z_threshold=DRIFT_Z_THRESHOLD\n",
        ")\n",
        "\n",
        "print(\"DriftMonitor initialized:\")\n",
        "print(f\"  Window size: {DRIFT_WINDOW_SIZE}\")\n",
        "print(f\"  KS alpha: {DRIFT_ALPHA}\")\n",
        "print(f\"  Z-threshold: {DRIFT_Z_THRESHOLD}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: Simulate Drift Scenario\n",
        "\n",
        "Demonstrate drift detection by simulating a change in model behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate drift by generating outputs with a modified prompt\n",
        "# This simulates what happens when the system prompt changes or model behavior shifts\n",
        "\n",
        "DRIFTED_SYSTEM_PROMPT = \"\"\"\n",
        "You are a casual AI chatbot that answers questions informally.\n",
        "\n",
        "Style guidelines:\n",
        "- Be conversational and friendly\n",
        "- Use simple language\n",
        "- Add emojis occasionally\n",
        "- Keep responses short and casual\n",
        "\"\"\"\n",
        "\n",
        "print(\"Simulating drift scenario...\")\n",
        "print(\"Generating outputs with MODIFIED system prompt to induce drift...\")\n",
        "\n",
        "# Generate a smaller set of \"drifted\" outputs\n",
        "N_DRIFT_SAMPLES = 20\n",
        "drifted_outputs = generate_batch_outputs(DRIFTED_SYSTEM_PROMPT, USER_PROMPT, N_DRIFT_SAMPLES, TEMPERATURE)\n",
        "\n",
        "# Embed drifted outputs\n",
        "drifted_embeddings = embed_texts(drifted_outputs)\n",
        "\n",
        "# Compute similarity to SAME reference (from baseline)\n",
        "drifted_sims = cosine_similarity(drifted_embeddings, ref_embedding.reshape(1, -1)).flatten()\n",
        "\n",
        "print(f\"\\nDrifted outputs statistics:\")\n",
        "print(f\"  Mean similarity: {np.mean(drifted_sims):.4f} (baseline: {baseline_stats['mean']:.4f})\")\n",
        "print(f\"  Std similarity:  {np.std(drifted_sims):.4f} (baseline: {baseline_stats['std']:.4f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run drift detection on the drifted samples\n",
        "print(\"Running drift detection...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# KS Test\n",
        "ks_result = detect_drift_ks(baseline_stats[\"scores\"], drifted_sims, DRIFT_ALPHA)\n",
        "print(f\"\\nKolmogorov-Smirnov Test:\")\n",
        "print(f\"  KS Statistic: {ks_result['ks_statistic']:.4f}\")\n",
        "print(f\"  P-value:      {ks_result['p_value']:.6f}\")\n",
        "print(f\"  Alpha:        {ks_result['alpha']}\")\n",
        "print(f\"  Drift Detected: {'YES' if ks_result['drift_detected'] else 'NO'}\")\n",
        "\n",
        "# Mean Shift\n",
        "mean_result = detect_drift_mean(baseline_stats[\"scores\"], drifted_sims, DRIFT_Z_THRESHOLD)\n",
        "print(f\"\\nMean Shift Test:\")\n",
        "print(f\"  Baseline Mean: {mean_result['baseline_mean']:.4f}\")\n",
        "print(f\"  New Mean:      {mean_result['new_mean']:.4f}\")\n",
        "print(f\"  Z-Score:       {mean_result['z_score']:.4f}\")\n",
        "print(f\"  Z-Threshold:   {mean_result['z_threshold']}\")\n",
        "print(f\"  Drift Detected: {'YES' if mean_result['drift_detected'] else 'NO'}\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "combined_drift = ks_result['drift_detected'] or mean_result['drift_detected']\n",
        "print(f\"\\nCOMBINED RESULT: {'DRIFT DETECTED - Recalibration recommended!' if combined_drift else 'No significant drift detected'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 12: Drift Detection Visualization\n",
        "\n",
        "Compare baseline and drifted distributions visually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize baseline vs drifted distributions\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Left: Histogram comparison\n",
        "ax1 = axes[0]\n",
        "ax1.hist(baseline_stats[\"scores\"], bins=20, alpha=0.6, color='#3498db', \n",
        "         label=f'Baseline (n={len(baseline_stats[\"scores\"])})', density=True)\n",
        "ax1.hist(drifted_sims, bins=15, alpha=0.6, color='#e74c3c', \n",
        "         label=f'Drifted (n={len(drifted_sims)})', density=True)\n",
        "ax1.axvline(baseline_stats[\"mean\"], color='#3498db', linestyle='--', linewidth=2, \n",
        "            label=f'Baseline Mean: {baseline_stats[\"mean\"]:.4f}')\n",
        "ax1.axvline(np.mean(drifted_sims), color='#e74c3c', linestyle='--', linewidth=2, \n",
        "            label=f'Drifted Mean: {np.mean(drifted_sims):.4f}')\n",
        "ax1.axvline(threshold, color='#2c3e50', linestyle='-', linewidth=2, \n",
        "            label=f'Threshold: {threshold:.4f}')\n",
        "ax1.set_xlabel('Cosine Similarity', fontsize=12)\n",
        "ax1.set_ylabel('Density', fontsize=12)\n",
        "ax1.set_title('Baseline vs Drifted Distribution', fontsize=14, fontweight='bold')\n",
        "ax1.legend(loc='upper left', fontsize=9)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Right: KDE overlay\n",
        "ax2 = axes[1]\n",
        "x_range = np.linspace(min(min(baseline_stats[\"scores\"]), min(drifted_sims)) - 0.05, \n",
        "                      max(max(baseline_stats[\"scores\"]), max(drifted_sims)) + 0.05, 200)\n",
        "\n",
        "kde_baseline = gaussian_kde(baseline_stats[\"scores\"])\n",
        "kde_drifted = gaussian_kde(drifted_sims)\n",
        "\n",
        "ax2.plot(x_range, kde_baseline(x_range), color='#3498db', linewidth=2.5, label='Baseline KDE')\n",
        "ax2.plot(x_range, kde_drifted(x_range), color='#e74c3c', linewidth=2.5, label='Drifted KDE')\n",
        "ax2.axvline(threshold, color='#2c3e50', linestyle='-', linewidth=2, label=f'Threshold: {threshold:.4f}')\n",
        "\n",
        "# Shade area below threshold for drifted distribution\n",
        "x_fill = x_range[x_range < threshold]\n",
        "ax2.fill_between(x_fill, kde_drifted(x_fill), alpha=0.3, color='#e74c3c', \n",
        "                 label='Below threshold (drifted)')\n",
        "\n",
        "ax2.set_xlabel('Cosine Similarity', fontsize=12)\n",
        "ax2.set_ylabel('Density', fontsize=12)\n",
        "ax2.set_title('KDE Comparison with Threshold', fontsize=14, fontweight='bold')\n",
        "ax2.legend(loc='upper left', fontsize=9)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate rejection rate\n",
        "baseline_rejection_rate = np.mean(baseline_stats[\"scores\"] < threshold) * 100\n",
        "drifted_rejection_rate = np.mean(drifted_sims < threshold) * 100\n",
        "\n",
        "print(f\"\\nRejection Rates:\")\n",
        "print(f\"  Baseline: {baseline_rejection_rate:.1f}%\")\n",
        "print(f\"  Drifted:  {drifted_rejection_rate:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 13: Recalibration Strategy\n",
        "\n",
        "Guidelines for when and how to recalibrate the threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recalibration Strategy Documentation\n",
        "recalibration_strategy = \"\"\"\n",
        "RECALIBRATION STRATEGY\n",
        "======================\n",
        "\n",
        "When to Recalibrate:\n",
        "-------------------\n",
        "1. Model version changes (OpenAI updates gpt-4o-mini)\n",
        "2. System prompt changes materially\n",
        "3. Embedding model changes (CRITICAL - thresholds become invalid!)\n",
        "4. Drift detection triggers consistently\n",
        "5. Observed acceptance/rejection rates shift significantly\n",
        "6. Periodic schedule (e.g., monthly for production systems)\n",
        "\n",
        "Dataset Split Strategy:\n",
        "----------------------\n",
        "When collecting new calibration data:\n",
        "- Calibration Set (60-70%): Used to compute threshold\n",
        "- Validation Set (20-30%): Used to verify threshold performance  \n",
        "- Holdout Set (10%): Never touched during calibration, used for final audit\n",
        "\n",
        "Important Notes:\n",
        "---------------\n",
        "- Do NOT use the same data for both drift detection and recalibration\n",
        "- Keep audit samples untouched to avoid overfitting\n",
        "- Log all calibration runs with timestamps for traceability\n",
        "- Store reference embeddings separately for version control\n",
        "\"\"\"\n",
        "\n",
        "print(recalibration_strategy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 14: Export Calibration Results\n",
        "\n",
        "Save calibration results for production use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Create calibration results dictionary\n",
        "calibration_results = {\n",
        "    \"metadata\": {\n",
        "        \"calibration_date\": datetime.now().isoformat(),\n",
        "        \"n_samples\": N_OUTPUTS,\n",
        "        \"generation_model\": GENERATION_MODEL,\n",
        "        \"embedding_model\": EMBEDDING_MODEL,\n",
        "        \"temperature\": TEMPERATURE\n",
        "    },\n",
        "    \"threshold\": {\n",
        "        \"value\": float(threshold),\n",
        "        \"percentile\": THRESHOLD_PERCENTILE,\n",
        "        \"ci_low\": float(ci_low),\n",
        "        \"ci_high\": float(ci_high),\n",
        "        \"confidence_level\": CONFIDENCE_LEVEL\n",
        "    },\n",
        "    \"baseline_statistics\": {\n",
        "        \"mean\": float(baseline_stats[\"mean\"]),\n",
        "        \"std\": float(baseline_stats[\"std\"]),\n",
        "        \"min\": float(np.min(baseline_stats[\"scores\"])),\n",
        "        \"max\": float(np.max(baseline_stats[\"scores\"]))\n",
        "    },\n",
        "    \"drift_detection_config\": {\n",
        "        \"window_size\": DRIFT_WINDOW_SIZE,\n",
        "        \"ks_alpha\": DRIFT_ALPHA,\n",
        "        \"z_threshold\": DRIFT_Z_THRESHOLD\n",
        "    }\n",
        "}\n",
        "\n",
        "# Display results\n",
        "print(\"CALIBRATION RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(json.dumps(calibration_results, indent=2))\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Optionally save to file\n",
        "# with open(\"calibration_results.json\", \"w\") as f:\n",
        "#     json.dump(calibration_results, f, indent=2)\n",
        "# print(\"\\nResults saved to calibration_results.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "Final summary of calibration results and operational recommendations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\"\"\n",
        "================================================================================\n",
        "                      CALIBRATION SUMMARY\n",
        "================================================================================\n",
        "\"\"\")\n",
        "\n",
        "print(f\"CALIBRATED THRESHOLD\")\n",
        "print(f\"  Value:            {threshold:.4f}\")\n",
        "print(f\"  95% CI:           [{ci_low:.4f}, {ci_high:.4f}]\")\n",
        "print(f\"  Percentile:       {THRESHOLD_PERCENTILE}th\")\n",
        "print()\n",
        "\n",
        "print(f\"BASELINE DISTRIBUTION\")\n",
        "print(f\"  Mean:             {baseline_stats['mean']:.4f}\")\n",
        "print(f\"  Std:              {baseline_stats['std']:.4f}\")\n",
        "print(f\"  N samples:        {len(baseline_stats['scores'])}\")\n",
        "print()\n",
        "\n",
        "print(f\"OPERATIONAL POLICY\")\n",
        "print(f\"  IF score >= {threshold:.4f}  --> ACCEPT\")\n",
        "print(f\"  IF score <  {threshold:.4f}  --> RETRY (up to 3 times)\")\n",
        "print(f\"  IF still below threshold  --> ESCALATE\")\n",
        "print()\n",
        "\n",
        "print(f\"DRIFT MONITORING\")\n",
        "print(f\"  Window size:      {DRIFT_WINDOW_SIZE} samples\")\n",
        "print(f\"  KS test alpha:    {DRIFT_ALPHA}\")\n",
        "print(f\"  Mean shift z:     {DRIFT_Z_THRESHOLD}\")\n",
        "print()\n",
        "\n",
        "print(\"\"\"\n",
        "NEXT STEPS:\n",
        "-----------\n",
        "1. Integrate threshold into production runtime\n",
        "2. Implement drift monitoring pipeline\n",
        "3. Set up alerting for drift detection\n",
        "4. Schedule periodic recalibration (e.g., monthly)\n",
        "5. Track acceptance/rejection rates over time\n",
        "\n",
        "IMPORTANT CAVEATS:\n",
        "-----------------\n",
        "- Cosine similarity alone won't catch format/structure violations\n",
        "- Add hard validators for JSON schema, required fields, etc.\n",
        "- Self-consistency != correctness (you can be consistently wrong)\n",
        "- Recalibrate when embedding model changes (thresholds invalidated!)\n",
        "================================================================================\n",
        "\"\"\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
